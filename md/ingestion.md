# RLM++ Ingestion & Summarization Logic

## Overview
This document explains the "Simultaneous Smart Chunking & Summarization" pipeline implemented in the RLM++ Indexer. This architecture is designed to handle large files efficiently by performing segmentation and initial summarization in a single pass, followed by a recursive hierarchical aggregation.

## Phase 1: The First Pass (Raw Context Parsing)

**Goal:** Transform a linear stream of raw text (e.g., a 2M token file) into discrete, overlapping semantic chunks, each with a corresponding "Level 0" summary.

### 1. The Rolling Buffer (Lookahead)
Instead of feeding the entire 2M token file to the LLM at once (which is costly and slow), the system uses a **Rolling Window** strategy.

*   **Cursor:** A pointer (`current_idx`) tracks our position in the file.
*   **Buffer Size:** The system grabs a segment of text starting from `current_idx`.
    *   This buffer provides the LLM with enough "lookahead" to find a natural breaking point (e.g., end of a chapter or topic) without needing to see the rest of the file.
    *   Adds a bit of a buffer (5000 max tokens)

### 2. Simultaneous Decision Making (The LLM Call)
The LLM receives this token buffer with three specific instructions to execute in parallel:

1.  **Find Cut Point:** Identify the best semantic stopping point relative to the start of the buffer (e.g., "Stop at character 14,500, after the paragraph about Neural Networks").
2.  **Determine Overlap:** Identify where the *next* chunk should logically start to maintain context (e.g., "Start next chunk at character 14,000"). This creates a 500-character overlap.
3.  **Summarize (Level 0):** Generate a concise summary of the text *specifically* from the start of the buffer up to the chosen Cut Point.

### 3. Immediate Storage
*   **Chunk:** The raw text `buffer[0 : cut_point]` is saved to the `chunks` table (ID: $N$).
*   **Summary:** The summary generated by the LLM is immediately saved to the `summaries` table (Level: 0), linked to Chunk ID $N$.

### 4. Advancing the Cursor
The system advances `current_idx` to the **Next Chunk Start** index determined by the LLM.
*   *Effect:* The next iteration's buffer will start slightly before the previous chunk ended, ensuring no information is lost at the boundaries.

---

## Phase 2: Hierarchical Summarization (Recursive Aggregation)

**Goal:** Create a "Tree of Knowledge" that allows the system to traverse from high-level concepts down to specific details.

Once Phase 1 is complete, we have a list of **Level 0 Summaries**. The system enters a recursive loop:

1.  **Grouping:** The summaries from the current level (starting with Level 0) are grouped into batches (defined by `group_size`, e.g., 5).
2.  **Synthesis:**
    *   The system takes a group of 5 summaries.
    *   It concatenates them into a single context.
    *   The LLM generates a **Level 1 Summary** that synthesizes these 5 points into a higher-level overview.
3.  **Storage & Linking:**
    *   The new Level 1 Summary is saved.
    *   The original 5 summaries are updated to point to this new Level 1 Summary as their `parent_id`.
4.  **Recursion:**
    *   This process repeats for Level 1 $	o$ Level 2, Level 2 $	o$ Level 3, etc.
    *   The loop terminates when only one "Root Summary" remains.

## Why This Logic?

1.  **Efficiency (1 Pass vs 2):** In previous iterations, we chunked the file (Pass 1) and then re-read every chunk to summarize it (Pass 2). By asking the LLM to "Cut & Summarize" simultaneously, we cut the input token costs and processing time significantly.
2.  **Context Awareness:** Because the LLM sees the text *after* the potential cut point (the lookahead), it can make better decisions about where a topic truly ends compared to a fixed-size splitter.
3.  **Safety:** The `target_chunk_tokens` cap ensures we never overflow the LLM's context window, even when processing massive files. The lookahead buffer is always a manageable slice of the whole.
